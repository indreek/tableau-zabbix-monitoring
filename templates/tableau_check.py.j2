#!/usr/bin/python3

import sys, os, time, requests,json, glob
import xml.etree.cElementTree as ET

CLEANUP_OLD_LOGS = {{ tableau_zabbix_cleanup_old_logs }}
URL = 'http://{{ tableau_zabbix_monitoring_server_address }}/admin/systeminfo.xml'
LOGDIR = '{{ tableau_zabbix_monitoring_log_dir }}'
STATUS_XML = os.path.join('{{ tableau_zabbix_monitoring_log_dir }}', '{{ tableau_zabbix_monitoring_status_xml }}')
LOGFILENAME = '%04d-%02d-%02dT%02d:%02d' % time.localtime(time.time())[:5]
LOGFILE = os.path.join(LOGDIR, LOGFILENAME)

if not os.path.exists(LOGDIR):
  os.makedirs(LOGDIR)

zbx_conf = '{{ tableau_zabbix_monitoring_zabbix_config_file }}'
zbx_discovery_machines = {"data":[]}
zbx_discovery_machine_key = 'MACHINE'
zbx_discovery_workers = {}
zbx_discovery_worker_key = 'WORKER'

tableau_status = {
  "Unknown": 0,
  "Active": 1,
  "Passive": 2,
  "Unlicensed": 3,
  "Busy": 4,
  "Down": 5,
  "ReadOnly": 6,
  "ActiveSyncing": 7,
  "StatusNotAvailable": 8,
  "StatusNotAvailableSyncing": 9,
  "NotAvailable": 10,
  "DecommisionedReadOnly": 11,
  "DecomisioningReadOnly": 12,
  "DecommissionFailedReadOnly": 13,
}

def zbx_sender(key, value):
  zbx_cmd = "zabbix_sender -c %s -k %s -o %s" % (zbx_conf, key, value)
  os.system(zbx_cmd)

#Request XML status
req = requests.get(URL)
file = open(STATUS_XML, 'wb')
for chunk in req.iter_content(100000):
  file.write(chunk)
file.close()

#Parse XML
tree = ET.parse(STATUS_XML)
root = tree.getroot()

#Send overall service status to zabbix
service_status = root.find('service').get('status')
zbx_sender('tableau.service.status', tableau_status[service_status])

#Send machines and services statuses to zabbix
for machine in root.find('machines'):
  machine_name = machine.get('name')
  {% raw %}zbx_discovery_machines['data'].append({('{#%s}' % zbx_discovery_machine_key):machine_name}){% endraw %}

  # Create machine overall status
  machine_service_statuses = []

  for service in machine:
    worker_id = service.get('worker')

    if service.tag not in zbx_discovery_workers:
      zbx_discovery_workers[service.tag] = { "data":[] }

    {% raw %}zbx_discovery_workers[service.tag]['data'].append({("{#%s}" % zbx_discovery_worker_key):worker_id}){% endraw %}

    for key, value in service.items():
      if key == 'worker':
        continue

      if key == 'status':
        #Convert machine status to integer
        if value in tableau_status:
          value = tableau_status[value]
        else:
          value = 0

        #Add service status to machine overall status
        machine_service_statuses.append(value)

      #Send service status to zabbix
      zbx_sender('tableau.%s[%s,%s]' % (service.tag, worker_id, key), value)

  #Send overall machine status to zabbix
  zbx_sender('tableau.machine[%s,status]' % (machine_name), max(machine_service_statuses))

#Write discovery data to file
f = open(LOGFILE, 'w')

#Machine discovery
f.write('MACHINE.discovery %s\n' % (json.dumps(zbx_discovery_machines)))
for worker in zbx_discovery_workers:
  #Worker discovery
  f.write('WORKER.%s.discovery %s\n' % (worker, json.dumps(zbx_discovery_workers[worker])))
f.close()

os.symlink(LOGFILE, os.path.join(LOGDIR, 'latest.tmp'))
os.rename(os.path.join(LOGDIR, 'latest.tmp'), os.path.join(LOGDIR, 'latest'))

if CLEANUP_OLD_LOGS:
    files = glob.glob(LOGDIR + '/*:*') # get all timestamp-named files
    files.sort()
    files.reverse()
    while len(files) > CLEANUP_OLD_LOGS:
        oldest_file = files.pop()
        os.remove(oldest_file)
